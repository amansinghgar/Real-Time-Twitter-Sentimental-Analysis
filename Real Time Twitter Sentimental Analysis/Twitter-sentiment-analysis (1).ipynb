{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split # function for splitting data to train and test sets\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from textblob import TextBlob\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Downloads\\trainingandtestdata\n"
     ]
    }
   ],
   "source": [
    "cd trainingandtestdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colnames=['polarity','tweet_ID','date','query','user','text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dftest=pd.read_csv('testdata.manual.2009.06.14.csv',names=colnames,encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dftrain=pd.read_csv('training.1600000.processed.noemoticon.csv',names=colnames,encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok = WordPunctTokenizer()\n",
    "\n",
    "pat1 = r'@[A-Za-z0-9_]+'\n",
    "pat2 = r'https?://[^ ]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "www_pat = r'www.[^ ]+'\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweet_cleaner_updated(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    \n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    stripped = re.sub(www_pat, '', stripped)\n",
    "    lower_case = stripped.lower()\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "    # During the letters_only process two lines above, it has created unnecessary white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n",
    "    \n",
    "    return (\" \".join(words)).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nums = [0,400000,800000,1200000,1600000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the tweets...\n",
      "\n",
      "Tweets 10000 of 400000 has been processed\n",
      "Tweets 20000 of 400000 has been processed\n",
      "Tweets 30000 of 400000 has been processed\n",
      "Tweets 40000 of 400000 has been processed\n",
      "Tweets 50000 of 400000 has been processed\n",
      "Tweets 60000 of 400000 has been processed\n",
      "Tweets 70000 of 400000 has been processed\n",
      "Tweets 80000 of 400000 has been processed\n",
      "Tweets 90000 of 400000 has been processed\n",
      "Tweets 100000 of 400000 has been processed\n",
      "Tweets 110000 of 400000 has been processed\n",
      "Tweets 120000 of 400000 has been processed\n",
      "Tweets 130000 of 400000 has been processed\n",
      "Tweets 140000 of 400000 has been processed\n",
      "Tweets 150000 of 400000 has been processed\n",
      "Tweets 160000 of 400000 has been processed\n",
      "Tweets 170000 of 400000 has been processed\n",
      "Tweets 180000 of 400000 has been processed\n",
      "Tweets 190000 of 400000 has been processed\n",
      "Tweets 200000 of 400000 has been processed\n",
      "Tweets 210000 of 400000 has been processed\n",
      "Tweets 220000 of 400000 has been processed\n",
      "Tweets 230000 of 400000 has been processed\n",
      "Tweets 240000 of 400000 has been processed\n",
      "Tweets 250000 of 400000 has been processed\n",
      "Tweets 260000 of 400000 has been processed\n",
      "Tweets 270000 of 400000 has been processed\n",
      "Tweets 280000 of 400000 has been processed\n",
      "Tweets 290000 of 400000 has been processed\n",
      "Tweets 300000 of 400000 has been processed\n",
      "Tweets 310000 of 400000 has been processed\n",
      "Tweets 320000 of 400000 has been processed\n",
      "Tweets 330000 of 400000 has been processed\n",
      "Tweets 340000 of 400000 has been processed\n",
      "Tweets 350000 of 400000 has been processed\n",
      "Tweets 360000 of 400000 has been processed\n",
      "Tweets 370000 of 400000 has been processed\n",
      "Tweets 380000 of 400000 has been processed\n",
      "Tweets 390000 of 400000 has been processed\n",
      "Tweets 400000 of 400000 has been processed\n",
      "Tweets 410000 of 800000 has been processed\n",
      "Tweets 420000 of 800000 has been processed\n",
      "Tweets 430000 of 800000 has been processed\n",
      "Tweets 440000 of 800000 has been processed\n",
      "Tweets 450000 of 800000 has been processed\n",
      "Tweets 460000 of 800000 has been processed\n",
      "Tweets 470000 of 800000 has been processed\n",
      "Tweets 480000 of 800000 has been processed\n",
      "Tweets 490000 of 800000 has been processed\n",
      "Tweets 500000 of 800000 has been processed\n",
      "Tweets 510000 of 800000 has been processed\n",
      "Tweets 520000 of 800000 has been processed\n",
      "Tweets 530000 of 800000 has been processed\n",
      "Tweets 540000 of 800000 has been processed\n",
      "Tweets 550000 of 800000 has been processed\n",
      "Tweets 560000 of 800000 has been processed\n",
      "Tweets 570000 of 800000 has been processed\n",
      "Tweets 580000 of 800000 has been processed\n",
      "Tweets 590000 of 800000 has been processed\n",
      "Tweets 600000 of 800000 has been processed\n",
      "Tweets 610000 of 800000 has been processed\n",
      "Tweets 620000 of 800000 has been processed\n",
      "Tweets 630000 of 800000 has been processed\n",
      "Tweets 640000 of 800000 has been processed\n",
      "Tweets 650000 of 800000 has been processed\n",
      "Tweets 660000 of 800000 has been processed\n",
      "Tweets 670000 of 800000 has been processed\n",
      "Tweets 680000 of 800000 has been processed\n",
      "Tweets 690000 of 800000 has been processed\n",
      "Tweets 700000 of 800000 has been processed\n",
      "Tweets 710000 of 800000 has been processed\n",
      "Tweets 720000 of 800000 has been processed\n",
      "Tweets 730000 of 800000 has been processed\n",
      "Tweets 740000 of 800000 has been processed\n",
      "Tweets 750000 of 800000 has been processed\n",
      "Tweets 760000 of 800000 has been processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:198: UserWarning: \"b' i just received my G8 viola exam.. and its... well... .. disappointing.. :\\\\..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets 770000 of 800000 has been processed\n",
      "Tweets 780000 of 800000 has been processed\n",
      "Tweets 790000 of 800000 has been processed\n",
      "Tweets 800000 of 800000 has been processed\n",
      "Tweets 810000 of 1200000 has been processed\n",
      "Tweets 820000 of 1200000 has been processed\n",
      "Tweets 830000 of 1200000 has been processed\n",
      "Tweets 840000 of 1200000 has been processed\n",
      "Tweets 850000 of 1200000 has been processed\n",
      "Tweets 860000 of 1200000 has been processed\n",
      "Tweets 870000 of 1200000 has been processed\n",
      "Tweets 880000 of 1200000 has been processed\n",
      "Tweets 890000 of 1200000 has been processed\n",
      "Tweets 900000 of 1200000 has been processed\n",
      "Tweets 910000 of 1200000 has been processed\n",
      "Tweets 920000 of 1200000 has been processed\n",
      "Tweets 930000 of 1200000 has been processed\n",
      "Tweets 940000 of 1200000 has been processed\n",
      "Tweets 950000 of 1200000 has been processed\n",
      "Tweets 960000 of 1200000 has been processed\n",
      "Tweets 970000 of 1200000 has been processed\n",
      "Tweets 980000 of 1200000 has been processed\n",
      "Tweets 990000 of 1200000 has been processed\n",
      "Tweets 1000000 of 1200000 has been processed\n",
      "Tweets 1010000 of 1200000 has been processed\n",
      "Tweets 1020000 of 1200000 has been processed\n",
      "Tweets 1030000 of 1200000 has been processed\n",
      "Tweets 1040000 of 1200000 has been processed\n",
      "Tweets 1050000 of 1200000 has been processed\n",
      "Tweets 1060000 of 1200000 has been processed\n",
      "Tweets 1070000 of 1200000 has been processed\n",
      "Tweets 1080000 of 1200000 has been processed\n",
      "Tweets 1090000 of 1200000 has been processed\n",
      "Tweets 1100000 of 1200000 has been processed\n",
      "Tweets 1110000 of 1200000 has been processed\n",
      "Tweets 1120000 of 1200000 has been processed\n",
      "Tweets 1130000 of 1200000 has been processed\n",
      "Tweets 1140000 of 1200000 has been processed\n",
      "Tweets 1150000 of 1200000 has been processed\n",
      "Tweets 1160000 of 1200000 has been processed\n",
      "Tweets 1170000 of 1200000 has been processed\n",
      "Tweets 1180000 of 1200000 has been processed\n",
      "Tweets 1190000 of 1200000 has been processed\n",
      "Tweets 1200000 of 1200000 has been processed\n",
      "Tweets 1210000 of 1600000 has been processed\n",
      "Tweets 1220000 of 1600000 has been processed\n",
      "Tweets 1230000 of 1600000 has been processed\n",
      "Tweets 1240000 of 1600000 has been processed\n",
      "Tweets 1250000 of 1600000 has been processed\n",
      "Tweets 1260000 of 1600000 has been processed\n",
      "Tweets 1270000 of 1600000 has been processed\n",
      "Tweets 1280000 of 1600000 has been processed\n",
      "Tweets 1290000 of 1600000 has been processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:198: UserWarning: \"b'E3 ON PLAYSTATION HOME IN ABOUT AN HOUR!!!!!!!!!! \\\\../  \\\\../'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets 1300000 of 1600000 has been processed\n",
      "Tweets 1310000 of 1600000 has been processed\n",
      "Tweets 1320000 of 1600000 has been processed\n",
      "Tweets 1330000 of 1600000 has been processed\n",
      "Tweets 1340000 of 1600000 has been processed\n",
      "Tweets 1350000 of 1600000 has been processed\n",
      "Tweets 1360000 of 1600000 has been processed\n",
      "Tweets 1370000 of 1600000 has been processed\n",
      "Tweets 1380000 of 1600000 has been processed\n",
      "Tweets 1390000 of 1600000 has been processed\n",
      "Tweets 1400000 of 1600000 has been processed\n",
      "Tweets 1410000 of 1600000 has been processed\n",
      "Tweets 1420000 of 1600000 has been processed\n",
      "Tweets 1430000 of 1600000 has been processed\n",
      "Tweets 1440000 of 1600000 has been processed\n",
      "Tweets 1450000 of 1600000 has been processed\n",
      "Tweets 1460000 of 1600000 has been processed\n",
      "Tweets 1470000 of 1600000 has been processed\n",
      "Tweets 1480000 of 1600000 has been processed\n",
      "Tweets 1490000 of 1600000 has been processed\n",
      "Tweets 1500000 of 1600000 has been processed\n",
      "Tweets 1510000 of 1600000 has been processed\n",
      "Tweets 1520000 of 1600000 has been processed\n",
      "Tweets 1530000 of 1600000 has been processed\n",
      "Tweets 1540000 of 1600000 has been processed\n",
      "Tweets 1550000 of 1600000 has been processed\n",
      "Tweets 1560000 of 1600000 has been processed\n",
      "Tweets 1570000 of 1600000 has been processed\n",
      "Tweets 1580000 of 1600000 has been processed\n",
      "Tweets 1590000 of 1600000 has been processed\n",
      "Tweets 1600000 of 1600000 has been processed\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaning and parsing the tweets...\\n\")\n",
    "clean_tweet_texts = []\n",
    "j=0\n",
    "while(j<4):\n",
    "    for i in range(nums[j],nums[j+1]):\n",
    "        if( (i+1)%10000 == 0 ):\n",
    "            print(\"Tweets %d of %d has been processed\" % ( i+1, nums[j+1] ) )                                                                   \n",
    "        clean_tweet_texts.append(tweet_cleaner_updated(dftrain['text'][i]))\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 498 entries, 0 to 497\n",
      "Data columns (total 6 columns):\n",
      "polarity    498 non-null int64\n",
      "tweet_ID    498 non-null int64\n",
      "date        498 non-null object\n",
      "query       498 non-null object\n",
      "user        498 non-null object\n",
      "text        498 non-null object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 23.4+ KB\n"
     ]
    }
   ],
   "source": [
    "dftest.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dftest0=dftest[dftest['polarity']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftest4=dftest[dftest['polarity']==4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dftestmod=pd.concat([dftest0,dftest4]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 359 entries, 0 to 358\n",
      "Data columns (total 7 columns):\n",
      "index       359 non-null int64\n",
      "polarity    359 non-null int64\n",
      "tweet_ID    359 non-null int64\n",
      "date        359 non-null object\n",
      "query       359 non-null object\n",
      "user        359 non-null object\n",
      "text        359 non-null object\n",
      "dtypes: int64(3), object(4)\n",
      "memory usage: 19.7+ KB\n"
     ]
    }
   ],
   "source": [
    "dftestmod.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the tweets...\n",
      "\n",
      "Tweets 1 of 359 has been processed\n",
      "Tweets 2 of 359 has been processed\n",
      "Tweets 3 of 359 has been processed\n",
      "Tweets 4 of 359 has been processed\n",
      "Tweets 5 of 359 has been processed\n",
      "Tweets 6 of 359 has been processed\n",
      "Tweets 7 of 359 has been processed\n",
      "Tweets 8 of 359 has been processed\n",
      "Tweets 9 of 359 has been processed\n",
      "Tweets 10 of 359 has been processed\n",
      "Tweets 11 of 359 has been processed\n",
      "Tweets 12 of 359 has been processed\n",
      "Tweets 13 of 359 has been processed\n",
      "Tweets 14 of 359 has been processed\n",
      "Tweets 15 of 359 has been processed\n",
      "Tweets 16 of 359 has been processed\n",
      "Tweets 17 of 359 has been processed\n",
      "Tweets 18 of 359 has been processed\n",
      "Tweets 19 of 359 has been processed\n",
      "Tweets 20 of 359 has been processed\n",
      "Tweets 21 of 359 has been processed\n",
      "Tweets 22 of 359 has been processed\n",
      "Tweets 23 of 359 has been processed\n",
      "Tweets 24 of 359 has been processed\n",
      "Tweets 25 of 359 has been processed\n",
      "Tweets 26 of 359 has been processed\n",
      "Tweets 27 of 359 has been processed\n",
      "Tweets 28 of 359 has been processed\n",
      "Tweets 29 of 359 has been processed\n",
      "Tweets 30 of 359 has been processed\n",
      "Tweets 31 of 359 has been processed\n",
      "Tweets 32 of 359 has been processed\n",
      "Tweets 33 of 359 has been processed\n",
      "Tweets 34 of 359 has been processed\n",
      "Tweets 35 of 359 has been processed\n",
      "Tweets 36 of 359 has been processed\n",
      "Tweets 37 of 359 has been processed\n",
      "Tweets 38 of 359 has been processed\n",
      "Tweets 39 of 359 has been processed\n",
      "Tweets 40 of 359 has been processed\n",
      "Tweets 41 of 359 has been processed\n",
      "Tweets 42 of 359 has been processed\n",
      "Tweets 43 of 359 has been processed\n",
      "Tweets 44 of 359 has been processed\n",
      "Tweets 45 of 359 has been processed\n",
      "Tweets 46 of 359 has been processed\n",
      "Tweets 47 of 359 has been processed\n",
      "Tweets 48 of 359 has been processed\n",
      "Tweets 49 of 359 has been processed\n",
      "Tweets 50 of 359 has been processed\n",
      "Tweets 51 of 359 has been processed\n",
      "Tweets 52 of 359 has been processed\n",
      "Tweets 53 of 359 has been processed\n",
      "Tweets 54 of 359 has been processed\n",
      "Tweets 55 of 359 has been processed\n",
      "Tweets 56 of 359 has been processed\n",
      "Tweets 57 of 359 has been processed\n",
      "Tweets 58 of 359 has been processed\n",
      "Tweets 59 of 359 has been processed\n",
      "Tweets 60 of 359 has been processed\n",
      "Tweets 61 of 359 has been processed\n",
      "Tweets 62 of 359 has been processed\n",
      "Tweets 63 of 359 has been processed\n",
      "Tweets 64 of 359 has been processed\n",
      "Tweets 65 of 359 has been processed\n",
      "Tweets 66 of 359 has been processed\n",
      "Tweets 67 of 359 has been processed\n",
      "Tweets 68 of 359 has been processed\n",
      "Tweets 69 of 359 has been processed\n",
      "Tweets 70 of 359 has been processed\n",
      "Tweets 71 of 359 has been processed\n",
      "Tweets 72 of 359 has been processed\n",
      "Tweets 73 of 359 has been processed\n",
      "Tweets 74 of 359 has been processed\n",
      "Tweets 75 of 359 has been processed\n",
      "Tweets 76 of 359 has been processed\n",
      "Tweets 77 of 359 has been processed\n",
      "Tweets 78 of 359 has been processed\n",
      "Tweets 79 of 359 has been processed\n",
      "Tweets 80 of 359 has been processed\n",
      "Tweets 81 of 359 has been processed\n",
      "Tweets 82 of 359 has been processed\n",
      "Tweets 83 of 359 has been processed\n",
      "Tweets 84 of 359 has been processed\n",
      "Tweets 85 of 359 has been processed\n",
      "Tweets 86 of 359 has been processed\n",
      "Tweets 87 of 359 has been processed\n",
      "Tweets 88 of 359 has been processed\n",
      "Tweets 89 of 359 has been processed\n",
      "Tweets 90 of 359 has been processed\n",
      "Tweets 91 of 359 has been processed\n",
      "Tweets 92 of 359 has been processed\n",
      "Tweets 93 of 359 has been processed\n",
      "Tweets 94 of 359 has been processed\n",
      "Tweets 95 of 359 has been processed\n",
      "Tweets 96 of 359 has been processed\n",
      "Tweets 97 of 359 has been processed\n",
      "Tweets 98 of 359 has been processed\n",
      "Tweets 99 of 359 has been processed\n",
      "Tweets 100 of 359 has been processed\n",
      "Tweets 101 of 359 has been processed\n",
      "Tweets 102 of 359 has been processed\n",
      "Tweets 103 of 359 has been processed\n",
      "Tweets 104 of 359 has been processed\n",
      "Tweets 105 of 359 has been processed\n",
      "Tweets 106 of 359 has been processed\n",
      "Tweets 107 of 359 has been processed\n",
      "Tweets 108 of 359 has been processed\n",
      "Tweets 109 of 359 has been processed\n",
      "Tweets 110 of 359 has been processed\n",
      "Tweets 111 of 359 has been processed\n",
      "Tweets 112 of 359 has been processed\n",
      "Tweets 113 of 359 has been processed\n",
      "Tweets 114 of 359 has been processed\n",
      "Tweets 115 of 359 has been processed\n",
      "Tweets 116 of 359 has been processed\n",
      "Tweets 117 of 359 has been processed\n",
      "Tweets 118 of 359 has been processed\n",
      "Tweets 119 of 359 has been processed\n",
      "Tweets 120 of 359 has been processed\n",
      "Tweets 121 of 359 has been processed\n",
      "Tweets 122 of 359 has been processed\n",
      "Tweets 123 of 359 has been processed\n",
      "Tweets 124 of 359 has been processed\n",
      "Tweets 125 of 359 has been processed\n",
      "Tweets 126 of 359 has been processed\n",
      "Tweets 127 of 359 has been processed\n",
      "Tweets 128 of 359 has been processed\n",
      "Tweets 129 of 359 has been processed\n",
      "Tweets 130 of 359 has been processed\n",
      "Tweets 131 of 359 has been processed\n",
      "Tweets 132 of 359 has been processed\n",
      "Tweets 133 of 359 has been processed\n",
      "Tweets 134 of 359 has been processed\n",
      "Tweets 135 of 359 has been processed\n",
      "Tweets 136 of 359 has been processed\n",
      "Tweets 137 of 359 has been processed\n",
      "Tweets 138 of 359 has been processed\n",
      "Tweets 139 of 359 has been processed\n",
      "Tweets 140 of 359 has been processed\n",
      "Tweets 141 of 359 has been processed\n",
      "Tweets 142 of 359 has been processed\n",
      "Tweets 143 of 359 has been processed\n",
      "Tweets 144 of 359 has been processed\n",
      "Tweets 145 of 359 has been processed\n",
      "Tweets 146 of 359 has been processed\n",
      "Tweets 147 of 359 has been processed\n",
      "Tweets 148 of 359 has been processed\n",
      "Tweets 149 of 359 has been processed\n",
      "Tweets 150 of 359 has been processed\n",
      "Tweets 151 of 359 has been processed\n",
      "Tweets 152 of 359 has been processed\n",
      "Tweets 153 of 359 has been processed\n",
      "Tweets 154 of 359 has been processed\n",
      "Tweets 155 of 359 has been processed\n",
      "Tweets 156 of 359 has been processed\n",
      "Tweets 157 of 359 has been processed\n",
      "Tweets 158 of 359 has been processed\n",
      "Tweets 159 of 359 has been processed\n",
      "Tweets 160 of 359 has been processed\n",
      "Tweets 161 of 359 has been processed\n",
      "Tweets 162 of 359 has been processed\n",
      "Tweets 163 of 359 has been processed\n",
      "Tweets 164 of 359 has been processed\n",
      "Tweets 165 of 359 has been processed\n",
      "Tweets 166 of 359 has been processed\n",
      "Tweets 167 of 359 has been processed\n",
      "Tweets 168 of 359 has been processed\n",
      "Tweets 169 of 359 has been processed\n",
      "Tweets 170 of 359 has been processed\n",
      "Tweets 171 of 359 has been processed\n",
      "Tweets 172 of 359 has been processed\n",
      "Tweets 173 of 359 has been processed\n",
      "Tweets 174 of 359 has been processed\n",
      "Tweets 175 of 359 has been processed\n",
      "Tweets 176 of 359 has been processed\n",
      "Tweets 177 of 359 has been processed\n",
      "Tweets 178 of 359 has been processed\n",
      "Tweets 179 of 359 has been processed\n",
      "Tweets 180 of 359 has been processed\n",
      "Tweets 181 of 359 has been processed\n",
      "Tweets 182 of 359 has been processed\n",
      "Tweets 183 of 359 has been processed\n",
      "Tweets 184 of 359 has been processed\n",
      "Tweets 185 of 359 has been processed\n",
      "Tweets 186 of 359 has been processed\n",
      "Tweets 187 of 359 has been processed\n",
      "Tweets 188 of 359 has been processed\n",
      "Tweets 189 of 359 has been processed\n",
      "Tweets 190 of 359 has been processed\n",
      "Tweets 191 of 359 has been processed\n",
      "Tweets 192 of 359 has been processed\n",
      "Tweets 193 of 359 has been processed\n",
      "Tweets 194 of 359 has been processed\n",
      "Tweets 195 of 359 has been processed\n",
      "Tweets 196 of 359 has been processed\n",
      "Tweets 197 of 359 has been processed\n",
      "Tweets 198 of 359 has been processed\n",
      "Tweets 199 of 359 has been processed\n",
      "Tweets 200 of 359 has been processed\n",
      "Tweets 201 of 359 has been processed\n",
      "Tweets 202 of 359 has been processed\n",
      "Tweets 203 of 359 has been processed\n",
      "Tweets 204 of 359 has been processed\n",
      "Tweets 205 of 359 has been processed\n",
      "Tweets 206 of 359 has been processed\n",
      "Tweets 207 of 359 has been processed\n",
      "Tweets 208 of 359 has been processed\n",
      "Tweets 209 of 359 has been processed\n",
      "Tweets 210 of 359 has been processed\n",
      "Tweets 211 of 359 has been processed\n",
      "Tweets 212 of 359 has been processed\n",
      "Tweets 213 of 359 has been processed\n",
      "Tweets 214 of 359 has been processed\n",
      "Tweets 215 of 359 has been processed\n",
      "Tweets 216 of 359 has been processed\n",
      "Tweets 217 of 359 has been processed\n",
      "Tweets 218 of 359 has been processed\n",
      "Tweets 219 of 359 has been processed\n",
      "Tweets 220 of 359 has been processed\n",
      "Tweets 221 of 359 has been processed\n",
      "Tweets 222 of 359 has been processed\n",
      "Tweets 223 of 359 has been processed\n",
      "Tweets 224 of 359 has been processed\n",
      "Tweets 225 of 359 has been processed\n",
      "Tweets 226 of 359 has been processed\n",
      "Tweets 227 of 359 has been processed\n",
      "Tweets 228 of 359 has been processed\n",
      "Tweets 229 of 359 has been processed\n",
      "Tweets 230 of 359 has been processed\n",
      "Tweets 231 of 359 has been processed\n",
      "Tweets 232 of 359 has been processed\n",
      "Tweets 233 of 359 has been processed\n",
      "Tweets 234 of 359 has been processed\n",
      "Tweets 235 of 359 has been processed\n",
      "Tweets 236 of 359 has been processed\n",
      "Tweets 237 of 359 has been processed\n",
      "Tweets 238 of 359 has been processed\n",
      "Tweets 239 of 359 has been processed\n",
      "Tweets 240 of 359 has been processed\n",
      "Tweets 241 of 359 has been processed\n",
      "Tweets 242 of 359 has been processed\n",
      "Tweets 243 of 359 has been processed\n",
      "Tweets 244 of 359 has been processed\n",
      "Tweets 245 of 359 has been processed\n",
      "Tweets 246 of 359 has been processed\n",
      "Tweets 247 of 359 has been processed\n",
      "Tweets 248 of 359 has been processed\n",
      "Tweets 249 of 359 has been processed\n",
      "Tweets 250 of 359 has been processed\n",
      "Tweets 251 of 359 has been processed\n",
      "Tweets 252 of 359 has been processed\n",
      "Tweets 253 of 359 has been processed\n",
      "Tweets 254 of 359 has been processed\n",
      "Tweets 255 of 359 has been processed\n",
      "Tweets 256 of 359 has been processed\n",
      "Tweets 257 of 359 has been processed\n",
      "Tweets 258 of 359 has been processed\n",
      "Tweets 259 of 359 has been processed\n",
      "Tweets 260 of 359 has been processed\n",
      "Tweets 261 of 359 has been processed\n",
      "Tweets 262 of 359 has been processed\n",
      "Tweets 263 of 359 has been processed\n",
      "Tweets 264 of 359 has been processed\n",
      "Tweets 265 of 359 has been processed\n",
      "Tweets 266 of 359 has been processed\n",
      "Tweets 267 of 359 has been processed\n",
      "Tweets 268 of 359 has been processed\n",
      "Tweets 269 of 359 has been processed\n",
      "Tweets 270 of 359 has been processed\n",
      "Tweets 271 of 359 has been processed\n",
      "Tweets 272 of 359 has been processed\n",
      "Tweets 273 of 359 has been processed\n",
      "Tweets 274 of 359 has been processed\n",
      "Tweets 275 of 359 has been processed\n",
      "Tweets 276 of 359 has been processed\n",
      "Tweets 277 of 359 has been processed\n",
      "Tweets 278 of 359 has been processed\n",
      "Tweets 279 of 359 has been processed\n",
      "Tweets 280 of 359 has been processed\n",
      "Tweets 281 of 359 has been processed\n",
      "Tweets 282 of 359 has been processed\n",
      "Tweets 283 of 359 has been processed\n",
      "Tweets 284 of 359 has been processed\n",
      "Tweets 285 of 359 has been processed\n",
      "Tweets 286 of 359 has been processed\n",
      "Tweets 287 of 359 has been processed\n",
      "Tweets 288 of 359 has been processed\n",
      "Tweets 289 of 359 has been processed\n",
      "Tweets 290 of 359 has been processed\n",
      "Tweets 291 of 359 has been processed\n",
      "Tweets 292 of 359 has been processed\n",
      "Tweets 293 of 359 has been processed\n",
      "Tweets 294 of 359 has been processed\n",
      "Tweets 295 of 359 has been processed\n",
      "Tweets 296 of 359 has been processed\n",
      "Tweets 297 of 359 has been processed\n",
      "Tweets 298 of 359 has been processed\n",
      "Tweets 299 of 359 has been processed\n",
      "Tweets 300 of 359 has been processed\n",
      "Tweets 301 of 359 has been processed\n",
      "Tweets 302 of 359 has been processed\n",
      "Tweets 303 of 359 has been processed\n",
      "Tweets 304 of 359 has been processed\n",
      "Tweets 305 of 359 has been processed\n",
      "Tweets 306 of 359 has been processed\n",
      "Tweets 307 of 359 has been processed\n",
      "Tweets 308 of 359 has been processed\n",
      "Tweets 309 of 359 has been processed\n",
      "Tweets 310 of 359 has been processed\n",
      "Tweets 311 of 359 has been processed\n",
      "Tweets 312 of 359 has been processed\n",
      "Tweets 313 of 359 has been processed\n",
      "Tweets 314 of 359 has been processed\n",
      "Tweets 315 of 359 has been processed\n",
      "Tweets 316 of 359 has been processed\n",
      "Tweets 317 of 359 has been processed\n",
      "Tweets 318 of 359 has been processed\n",
      "Tweets 319 of 359 has been processed\n",
      "Tweets 320 of 359 has been processed\n",
      "Tweets 321 of 359 has been processed\n",
      "Tweets 322 of 359 has been processed\n",
      "Tweets 323 of 359 has been processed\n",
      "Tweets 324 of 359 has been processed\n",
      "Tweets 325 of 359 has been processed\n",
      "Tweets 326 of 359 has been processed\n",
      "Tweets 327 of 359 has been processed\n",
      "Tweets 328 of 359 has been processed\n",
      "Tweets 329 of 359 has been processed\n",
      "Tweets 330 of 359 has been processed\n",
      "Tweets 331 of 359 has been processed\n",
      "Tweets 332 of 359 has been processed\n",
      "Tweets 333 of 359 has been processed\n",
      "Tweets 334 of 359 has been processed\n",
      "Tweets 335 of 359 has been processed\n",
      "Tweets 336 of 359 has been processed\n",
      "Tweets 337 of 359 has been processed\n",
      "Tweets 338 of 359 has been processed\n",
      "Tweets 339 of 359 has been processed\n",
      "Tweets 340 of 359 has been processed\n",
      "Tweets 341 of 359 has been processed\n",
      "Tweets 342 of 359 has been processed\n",
      "Tweets 343 of 359 has been processed\n",
      "Tweets 344 of 359 has been processed\n",
      "Tweets 345 of 359 has been processed\n",
      "Tweets 346 of 359 has been processed\n",
      "Tweets 347 of 359 has been processed\n",
      "Tweets 348 of 359 has been processed\n",
      "Tweets 349 of 359 has been processed\n",
      "Tweets 350 of 359 has been processed\n",
      "Tweets 351 of 359 has been processed\n",
      "Tweets 352 of 359 has been processed\n",
      "Tweets 353 of 359 has been processed\n",
      "Tweets 354 of 359 has been processed\n",
      "Tweets 355 of 359 has been processed\n",
      "Tweets 356 of 359 has been processed\n",
      "Tweets 357 of 359 has been processed\n",
      "Tweets 358 of 359 has been processed\n",
      "Tweets 359 of 359 has been processed\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaning and parsing the tweets...\\n\")\n",
    "clean_tweet_test_texts = []\n",
    "\n",
    "\n",
    "for i in range(359):\n",
    "        \n",
    "    print(\"Tweets %d of 359 has been processed\" % ( i+1 ) )                                                                   \n",
    "    clean_tweet_test_texts.append(tweet_cleaner_updated(dftestmod['text'][i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test=clean_tweet_test_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=clean_tweet_texts\n",
    "y=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y=[0 if i<800000 else 4 for i in range(1600000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tbresult = [TextBlob(i).sentiment.polarity for i in X]\n",
    "tbpred = [0 if n < 0 else 1 for n in tbresult]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matrix=confusion_matrix(y, tbpred, labels=[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test=dftestmod.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf=pd.DataFrame(matrix, index=['positive', 'negative'],\n",
    "                         columns=['predicted_positive','predicted_negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_positive</th>\n",
       "      <th>predicted_negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>719934</td>\n",
       "      <td>80066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>541149</td>\n",
       "      <td>258851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          predicted_positive  predicted_negative\n",
       "positive              719934               80066\n",
       "negative              541149              258851"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.174062500000005"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y, tbpred)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.7887325\n",
      "Accuracy for C=0.05: 0.7965825\n",
      "Accuracy for C=0.25: 0.79964\n",
      "Accuracy for C=0.5: 0.7993475\n",
      "Accuracy for C=1: 0.7984025\n"
     ]
    }
   ],
   "source": [
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftestmod=pd.concat([dftest0,dftest4]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=80000, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for n in range(10000,1000001,10000):\n",
    "vectorizer.set_params(max_features=80000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vectorizer.fit(X)\n",
    "X_2 = vectorizer.transform(X)\n",
    "X_test_2 = vectorizer.transform(X_test)\n",
    "\n",
    "X_train_2, X_val_2, y_train_2, y_val_2 = train_test_split(\n",
    "    X_2, dftrain.polarity, train_size = 0.99\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.5: 0.7975\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C=0.5)\n",
    "lr.fit(X_train_2, y_train_2)\n",
    "print (\"Accuracy for C=%s: %s\" % (0.5, accuracy_score(y_val_2, lr.predict(X_val_2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.824512534819\n"
     ]
    }
   ],
   "source": [
    "final_model = LogisticRegression(C=0.5)\n",
    "final_model.fit(X_2, y)\n",
    "\n",
    "print (\"Final Accuracy: %s\" % accuracy_score(y_test, final_model.predict(X_test_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=80000, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words={'about', 'were', 'the', 'been', 'for', \"weren't\", 'until', 'up', 'now', 'd', 'does', \"hasn't\", 'very', 'her', 'shouldn', 'with', 'then', 'whom', 'no', \"you'd\", 'after', 'me', 'there', 'both', 'is', \"you've\", 'are', 'mightn', 'can', 'but', 'over', \"hadn't\", 'mustn', 'just', 'himself', 'th... 'having', 'as', \"wasn't\", 'so', 'our', 'weren', 'theirs', 'haven', 'was', 'she', 'hasn', 'herself'},\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for n in range(10000,1000001,10000):\n",
    "vectorizer.set_params(max_features=80000,stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vectorizer.fit(X)\n",
    "X_2 = vectorizer.transform(X)\n",
    "X_test_2 = vectorizer.transform(X_test)\n",
    "\n",
    "X_train_2, X_val_2, y_train_2, y_val_2 = train_test_split(\n",
    "    X_2, dftrain.polarity, train_size = 0.99\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.5: 0.7793125\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C=0.5)\n",
    "lr.fit(X_train_2, y_train_2)\n",
    "print (\"Accuracy for C=%s: %s\" % (0.5, accuracy_score(y_val_2, lr.predict(X_val_2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.796657381616\n"
     ]
    }
   ],
   "source": [
    "final_model = LogisticRegression(C=0.5)\n",
    "final_model.fit(X_2, y)\n",
    "\n",
    "print (\"Final Accuracy: %s\" % accuracy_score(y_test, final_model.predict(X_test_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.5: 0.7938125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.5: 0.7971875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.5: 0.79775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.5: 0.7930625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.5: 0.79725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.5: 0.7989375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.5: 0.8024375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.5: 0.8051875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.5: 0.7974375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.5: 0.797375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.5: 0.8029375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-116-c47f0f052c45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     )\n\u001b[0;32m     10\u001b[0m     \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy for C=%s: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1231\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1232\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1233\u001b[1;33m                 sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m   1234\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 890\u001b[1;33m         epsilon, sample_weight)\n\u001b[0m\u001b[0;32m    891\u001b[0m     \u001b[1;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m     \u001b[1;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for n in range(10000,1000001,10000):\n",
    "    vectorizer.set_params(max_features=n)\n",
    "    vectorizer.fit(X)\n",
    "    X_2 = vectorizer.transform(X)\n",
    "    X_test_2 = vectorizer.transform(X_test)\n",
    "\n",
    "    X_train_2, X_val_2, y_train_2, y_val_2 = train_test_split(\n",
    "        X_2, dftrain.polarity, train_size = 0.99\n",
    "    )\n",
    "    lr = LogisticRegression(C=0.5)\n",
    "    lr.fit(X_train_2, y_train_2)\n",
    "    print (\"Accuracy for C=%s: %s\" % (0.5, accuracy_score(y_val_2, lr.predict(X_val_2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok = WordPunctTokenizer()\n",
    "\n",
    "pat1 = r'@[A-Za-z0-9_]+'\n",
    "pat2 = r'https?://[^ ]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "www_pat = r'www.[^ ]+'\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweet_cleaner_updated2(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    stripped = re.sub(combined_pat, '', bom_removed)\n",
    "    stripped = re.sub(www_pat, '', stripped)\n",
    "    lower_case = stripped.lower()\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n",
    "    \n",
    "    tweet=\" \".join(words).strip()\n",
    "    return TextBlob(tweet).correct().string\n",
    "    #return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nums = [0,400000,800000,1200000,1600000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the tweets...\n",
      "\n",
      "Negative Tweets 100 has been processed\n",
      "Negative Tweets 200 has been processed\n",
      "Negative Tweets 300 has been processed\n",
      "Negative Tweets 400 has been processed\n",
      "Negative Tweets 500 has been processed\n",
      "Negative Tweets 600 has been processed\n",
      "Negative Tweets 700 has been processed\n",
      "Negative Tweets 800 has been processed\n",
      "Negative Tweets 900 has been processed\n",
      "Negative Tweets 1000 has been processed\n",
      "Negative Tweets 1100 has been processed\n",
      "Negative Tweets 1200 has been processed\n",
      "Negative Tweets 1300 has been processed\n",
      "Negative Tweets 1400 has been processed\n",
      "Negative Tweets 1500 has been processed\n",
      "Negative Tweets 1600 has been processed\n",
      "Negative Tweets 1700 has been processed\n",
      "Negative Tweets 1800 has been processed\n",
      "Negative Tweets 1900 has been processed\n",
      "Negative Tweets 2000 has been processed\n",
      "Negative Tweets 2100 has been processed\n",
      "Negative Tweets 2200 has been processed\n",
      "Negative Tweets 2300 has been processed\n",
      "Negative Tweets 2400 has been processed\n",
      "Negative Tweets 2500 has been processed\n",
      "Negative Tweets 2600 has been processed\n",
      "Negative Tweets 2700 has been processed\n",
      "Negative Tweets 2800 has been processed\n",
      "Negative Tweets 2900 has been processed\n",
      "Negative Tweets 3000 has been processed\n",
      "Negative Tweets 3100 has been processed\n",
      "Negative Tweets 3200 has been processed\n",
      "Negative Tweets 3300 has been processed\n",
      "Negative Tweets 3400 has been processed\n",
      "Negative Tweets 3500 has been processed\n",
      "Negative Tweets 3600 has been processed\n",
      "Negative Tweets 3700 has been processed\n",
      "Negative Tweets 3800 has been processed\n",
      "Negative Tweets 3900 has been processed\n",
      "Negative Tweets 4000 has been processed\n",
      "Negative Tweets 4100 has been processed\n",
      "Negative Tweets 4200 has been processed\n",
      "Negative Tweets 4300 has been processed\n",
      "Negative Tweets 4400 has been processed\n",
      "Negative Tweets 4500 has been processed\n",
      "Negative Tweets 4600 has been processed\n",
      "Negative Tweets 4700 has been processed\n",
      "Negative Tweets 4800 has been processed\n",
      "Negative Tweets 4900 has been processed\n",
      "Negative Tweets 5000 has been processed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Cleaning and parsing the tweets...\\n\")\n",
    "clean_tweet_texts = []\n",
    "\n",
    "for i in range(5000):\n",
    "    if( (i+1)%100 == 0 ):\n",
    "        print(\"Negative Tweets %d has been processed\" % ( i+1) )                                                                   \n",
    "    clean_tweet_texts.append(tweet_cleaner_updated2(dftrain['text'][i]))\n",
    "len(clean_tweet_texts)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Tweets 100  has been processed\n",
      "Positive Tweets 200  has been processed\n",
      "Positive Tweets 300  has been processed\n",
      "Positive Tweets 400  has been processed\n",
      "Positive Tweets 500  has been processed\n",
      "Positive Tweets 600  has been processed\n",
      "Positive Tweets 700  has been processed\n",
      "Positive Tweets 800  has been processed\n",
      "Positive Tweets 900  has been processed\n",
      "Positive Tweets 1000  has been processed\n",
      "Positive Tweets 1100  has been processed\n",
      "Positive Tweets 1200  has been processed\n",
      "Positive Tweets 1300  has been processed\n",
      "Positive Tweets 1400  has been processed\n",
      "Positive Tweets 1500  has been processed\n",
      "Positive Tweets 1600  has been processed\n",
      "Positive Tweets 1700  has been processed\n",
      "Positive Tweets 1800  has been processed\n",
      "Positive Tweets 1900  has been processed\n",
      "Positive Tweets 2000  has been processed\n",
      "Positive Tweets 2100  has been processed\n",
      "Positive Tweets 2200  has been processed\n",
      "Positive Tweets 2300  has been processed\n",
      "Positive Tweets 2400  has been processed\n",
      "Positive Tweets 2500  has been processed\n",
      "Positive Tweets 2600  has been processed\n",
      "Positive Tweets 2700  has been processed\n",
      "Positive Tweets 2800  has been processed\n",
      "Positive Tweets 2900  has been processed\n",
      "Positive Tweets 3000  has been processed\n",
      "Positive Tweets 3100  has been processed\n",
      "Positive Tweets 3200  has been processed\n",
      "Positive Tweets 3300  has been processed\n",
      "Positive Tweets 3400  has been processed\n",
      "Positive Tweets 3500  has been processed\n",
      "Positive Tweets 3600  has been processed\n",
      "Positive Tweets 3700  has been processed\n",
      "Positive Tweets 3800  has been processed\n",
      "Positive Tweets 3900  has been processed\n",
      "Positive Tweets 4000  has been processed\n",
      "Positive Tweets 4100  has been processed\n",
      "Positive Tweets 4200  has been processed\n",
      "Positive Tweets 4300  has been processed\n",
      "Positive Tweets 4400  has been processed\n",
      "Positive Tweets 4500  has been processed\n",
      "Positive Tweets 4600  has been processed\n",
      "Positive Tweets 4700  has been processed\n",
      "Positive Tweets 4800  has been processed\n",
      "Positive Tweets 4900  has been processed\n",
      "Positive Tweets 5000  has been processed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for i in range(5000):\n",
    "    if( (i+1)%100 == 0 ):\n",
    "        print(\"Positive Tweets %d  has been processed\" % ( i+1 ) )                                                                   \n",
    "    clean_tweet_texts.append(tweet_cleaner_updated2(dftrain['text'][1599999-i]))\n",
    "len(clean_tweet_texts)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 498 entries, 0 to 497\n",
      "Data columns (total 6 columns):\n",
      "polarity    498 non-null int64\n",
      "tweet_ID    498 non-null int64\n",
      "date        498 non-null object\n",
      "query       498 non-null object\n",
      "user        498 non-null object\n",
      "text        498 non-null object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 23.4+ KB\n"
     ]
    }
   ],
   "source": [
    "dftest.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_tweet_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dftest0=dftest[dftest['polarity']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftest4=dftest[dftest['polarity']==4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dftestmod=pd.concat([dftest0,dftest4]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 359 entries, 0 to 358\n",
      "Data columns (total 7 columns):\n",
      "index       359 non-null int64\n",
      "polarity    359 non-null int64\n",
      "tweet_ID    359 non-null int64\n",
      "date        359 non-null object\n",
      "query       359 non-null object\n",
      "user        359 non-null object\n",
      "text        359 non-null object\n",
      "dtypes: int64(3), object(4)\n",
      "memory usage: 19.7+ KB\n"
     ]
    }
   ],
   "source": [
    "dftestmod.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the tweets...\n",
      "\n",
      "Tweets 1 of 359 has been processed\n",
      "Tweets 2 of 359 has been processed\n",
      "Tweets 3 of 359 has been processed\n",
      "Tweets 4 of 359 has been processed\n",
      "Tweets 5 of 359 has been processed\n",
      "Tweets 6 of 359 has been processed\n",
      "Tweets 7 of 359 has been processed\n",
      "Tweets 8 of 359 has been processed\n",
      "Tweets 9 of 359 has been processed\n",
      "Tweets 10 of 359 has been processed\n",
      "Tweets 11 of 359 has been processed\n",
      "Tweets 12 of 359 has been processed\n",
      "Tweets 13 of 359 has been processed\n",
      "Tweets 14 of 359 has been processed\n",
      "Tweets 15 of 359 has been processed\n",
      "Tweets 16 of 359 has been processed\n",
      "Tweets 17 of 359 has been processed\n",
      "Tweets 18 of 359 has been processed\n",
      "Tweets 19 of 359 has been processed\n",
      "Tweets 20 of 359 has been processed\n",
      "Tweets 21 of 359 has been processed\n",
      "Tweets 22 of 359 has been processed\n",
      "Tweets 23 of 359 has been processed\n",
      "Tweets 24 of 359 has been processed\n",
      "Tweets 25 of 359 has been processed\n",
      "Tweets 26 of 359 has been processed\n",
      "Tweets 27 of 359 has been processed\n",
      "Tweets 28 of 359 has been processed\n",
      "Tweets 29 of 359 has been processed\n",
      "Tweets 30 of 359 has been processed\n",
      "Tweets 31 of 359 has been processed\n",
      "Tweets 32 of 359 has been processed\n",
      "Tweets 33 of 359 has been processed\n",
      "Tweets 34 of 359 has been processed\n",
      "Tweets 35 of 359 has been processed\n",
      "Tweets 36 of 359 has been processed\n",
      "Tweets 37 of 359 has been processed\n",
      "Tweets 38 of 359 has been processed\n",
      "Tweets 39 of 359 has been processed\n",
      "Tweets 40 of 359 has been processed\n",
      "Tweets 41 of 359 has been processed\n",
      "Tweets 42 of 359 has been processed\n",
      "Tweets 43 of 359 has been processed\n",
      "Tweets 44 of 359 has been processed\n",
      "Tweets 45 of 359 has been processed\n",
      "Tweets 46 of 359 has been processed\n",
      "Tweets 47 of 359 has been processed\n",
      "Tweets 48 of 359 has been processed\n",
      "Tweets 49 of 359 has been processed\n",
      "Tweets 50 of 359 has been processed\n",
      "Tweets 51 of 359 has been processed\n",
      "Tweets 52 of 359 has been processed\n",
      "Tweets 53 of 359 has been processed\n",
      "Tweets 54 of 359 has been processed\n",
      "Tweets 55 of 359 has been processed\n",
      "Tweets 56 of 359 has been processed\n",
      "Tweets 57 of 359 has been processed\n",
      "Tweets 58 of 359 has been processed\n",
      "Tweets 59 of 359 has been processed\n",
      "Tweets 60 of 359 has been processed\n",
      "Tweets 61 of 359 has been processed\n",
      "Tweets 62 of 359 has been processed\n",
      "Tweets 63 of 359 has been processed\n",
      "Tweets 64 of 359 has been processed\n",
      "Tweets 65 of 359 has been processed\n",
      "Tweets 66 of 359 has been processed\n",
      "Tweets 67 of 359 has been processed\n",
      "Tweets 68 of 359 has been processed\n",
      "Tweets 69 of 359 has been processed\n",
      "Tweets 70 of 359 has been processed\n",
      "Tweets 71 of 359 has been processed\n",
      "Tweets 72 of 359 has been processed\n",
      "Tweets 73 of 359 has been processed\n",
      "Tweets 74 of 359 has been processed\n",
      "Tweets 75 of 359 has been processed\n",
      "Tweets 76 of 359 has been processed\n",
      "Tweets 77 of 359 has been processed\n",
      "Tweets 78 of 359 has been processed\n",
      "Tweets 79 of 359 has been processed\n",
      "Tweets 80 of 359 has been processed\n",
      "Tweets 81 of 359 has been processed\n",
      "Tweets 82 of 359 has been processed\n",
      "Tweets 83 of 359 has been processed\n",
      "Tweets 84 of 359 has been processed\n",
      "Tweets 85 of 359 has been processed\n",
      "Tweets 86 of 359 has been processed\n",
      "Tweets 87 of 359 has been processed\n",
      "Tweets 88 of 359 has been processed\n",
      "Tweets 89 of 359 has been processed\n",
      "Tweets 90 of 359 has been processed\n",
      "Tweets 91 of 359 has been processed\n",
      "Tweets 92 of 359 has been processed\n",
      "Tweets 93 of 359 has been processed\n",
      "Tweets 94 of 359 has been processed\n",
      "Tweets 95 of 359 has been processed\n",
      "Tweets 96 of 359 has been processed\n",
      "Tweets 97 of 359 has been processed\n",
      "Tweets 98 of 359 has been processed\n",
      "Tweets 99 of 359 has been processed\n",
      "Tweets 100 of 359 has been processed\n",
      "Tweets 101 of 359 has been processed\n",
      "Tweets 102 of 359 has been processed\n",
      "Tweets 103 of 359 has been processed\n",
      "Tweets 104 of 359 has been processed\n",
      "Tweets 105 of 359 has been processed\n",
      "Tweets 106 of 359 has been processed\n",
      "Tweets 107 of 359 has been processed\n",
      "Tweets 108 of 359 has been processed\n",
      "Tweets 109 of 359 has been processed\n",
      "Tweets 110 of 359 has been processed\n",
      "Tweets 111 of 359 has been processed\n",
      "Tweets 112 of 359 has been processed\n",
      "Tweets 113 of 359 has been processed\n",
      "Tweets 114 of 359 has been processed\n",
      "Tweets 115 of 359 has been processed\n",
      "Tweets 116 of 359 has been processed\n",
      "Tweets 117 of 359 has been processed\n",
      "Tweets 118 of 359 has been processed\n",
      "Tweets 119 of 359 has been processed\n",
      "Tweets 120 of 359 has been processed\n",
      "Tweets 121 of 359 has been processed\n",
      "Tweets 122 of 359 has been processed\n",
      "Tweets 123 of 359 has been processed\n",
      "Tweets 124 of 359 has been processed\n",
      "Tweets 125 of 359 has been processed\n",
      "Tweets 126 of 359 has been processed\n",
      "Tweets 127 of 359 has been processed\n",
      "Tweets 128 of 359 has been processed\n",
      "Tweets 129 of 359 has been processed\n",
      "Tweets 130 of 359 has been processed\n",
      "Tweets 131 of 359 has been processed\n",
      "Tweets 132 of 359 has been processed\n",
      "Tweets 133 of 359 has been processed\n",
      "Tweets 134 of 359 has been processed\n",
      "Tweets 135 of 359 has been processed\n",
      "Tweets 136 of 359 has been processed\n",
      "Tweets 137 of 359 has been processed\n",
      "Tweets 138 of 359 has been processed\n",
      "Tweets 139 of 359 has been processed\n",
      "Tweets 140 of 359 has been processed\n",
      "Tweets 141 of 359 has been processed\n",
      "Tweets 142 of 359 has been processed\n",
      "Tweets 143 of 359 has been processed\n",
      "Tweets 144 of 359 has been processed\n",
      "Tweets 145 of 359 has been processed\n",
      "Tweets 146 of 359 has been processed\n",
      "Tweets 147 of 359 has been processed\n",
      "Tweets 148 of 359 has been processed\n",
      "Tweets 149 of 359 has been processed\n",
      "Tweets 150 of 359 has been processed\n",
      "Tweets 151 of 359 has been processed\n",
      "Tweets 152 of 359 has been processed\n",
      "Tweets 153 of 359 has been processed\n",
      "Tweets 154 of 359 has been processed\n",
      "Tweets 155 of 359 has been processed\n",
      "Tweets 156 of 359 has been processed\n",
      "Tweets 157 of 359 has been processed\n",
      "Tweets 158 of 359 has been processed\n",
      "Tweets 159 of 359 has been processed\n",
      "Tweets 160 of 359 has been processed\n",
      "Tweets 161 of 359 has been processed\n",
      "Tweets 162 of 359 has been processed\n",
      "Tweets 163 of 359 has been processed\n",
      "Tweets 164 of 359 has been processed\n",
      "Tweets 165 of 359 has been processed\n",
      "Tweets 166 of 359 has been processed\n",
      "Tweets 167 of 359 has been processed\n",
      "Tweets 168 of 359 has been processed\n",
      "Tweets 169 of 359 has been processed\n",
      "Tweets 170 of 359 has been processed\n",
      "Tweets 171 of 359 has been processed\n",
      "Tweets 172 of 359 has been processed\n",
      "Tweets 173 of 359 has been processed\n",
      "Tweets 174 of 359 has been processed\n",
      "Tweets 175 of 359 has been processed\n",
      "Tweets 176 of 359 has been processed\n",
      "Tweets 177 of 359 has been processed\n",
      "Tweets 178 of 359 has been processed\n",
      "Tweets 179 of 359 has been processed\n",
      "Tweets 180 of 359 has been processed\n",
      "Tweets 181 of 359 has been processed\n",
      "Tweets 182 of 359 has been processed\n",
      "Tweets 183 of 359 has been processed\n",
      "Tweets 184 of 359 has been processed\n",
      "Tweets 185 of 359 has been processed\n",
      "Tweets 186 of 359 has been processed\n",
      "Tweets 187 of 359 has been processed\n",
      "Tweets 188 of 359 has been processed\n",
      "Tweets 189 of 359 has been processed\n",
      "Tweets 190 of 359 has been processed\n",
      "Tweets 191 of 359 has been processed\n",
      "Tweets 192 of 359 has been processed\n",
      "Tweets 193 of 359 has been processed\n",
      "Tweets 194 of 359 has been processed\n",
      "Tweets 195 of 359 has been processed\n",
      "Tweets 196 of 359 has been processed\n",
      "Tweets 197 of 359 has been processed\n",
      "Tweets 198 of 359 has been processed\n",
      "Tweets 199 of 359 has been processed\n",
      "Tweets 200 of 359 has been processed\n",
      "Tweets 201 of 359 has been processed\n",
      "Tweets 202 of 359 has been processed\n",
      "Tweets 203 of 359 has been processed\n",
      "Tweets 204 of 359 has been processed\n",
      "Tweets 205 of 359 has been processed\n",
      "Tweets 206 of 359 has been processed\n",
      "Tweets 207 of 359 has been processed\n",
      "Tweets 208 of 359 has been processed\n",
      "Tweets 209 of 359 has been processed\n",
      "Tweets 210 of 359 has been processed\n",
      "Tweets 211 of 359 has been processed\n",
      "Tweets 212 of 359 has been processed\n",
      "Tweets 213 of 359 has been processed\n",
      "Tweets 214 of 359 has been processed\n",
      "Tweets 215 of 359 has been processed\n",
      "Tweets 216 of 359 has been processed\n",
      "Tweets 217 of 359 has been processed\n",
      "Tweets 218 of 359 has been processed\n",
      "Tweets 219 of 359 has been processed\n",
      "Tweets 220 of 359 has been processed\n",
      "Tweets 221 of 359 has been processed\n",
      "Tweets 222 of 359 has been processed\n",
      "Tweets 223 of 359 has been processed\n",
      "Tweets 224 of 359 has been processed\n",
      "Tweets 225 of 359 has been processed\n",
      "Tweets 226 of 359 has been processed\n",
      "Tweets 227 of 359 has been processed\n",
      "Tweets 228 of 359 has been processed\n",
      "Tweets 229 of 359 has been processed\n",
      "Tweets 230 of 359 has been processed\n",
      "Tweets 231 of 359 has been processed\n",
      "Tweets 232 of 359 has been processed\n",
      "Tweets 233 of 359 has been processed\n",
      "Tweets 234 of 359 has been processed\n",
      "Tweets 235 of 359 has been processed\n",
      "Tweets 236 of 359 has been processed\n",
      "Tweets 237 of 359 has been processed\n",
      "Tweets 238 of 359 has been processed\n",
      "Tweets 239 of 359 has been processed\n",
      "Tweets 240 of 359 has been processed\n",
      "Tweets 241 of 359 has been processed\n",
      "Tweets 242 of 359 has been processed\n",
      "Tweets 243 of 359 has been processed\n",
      "Tweets 244 of 359 has been processed\n",
      "Tweets 245 of 359 has been processed\n",
      "Tweets 246 of 359 has been processed\n",
      "Tweets 247 of 359 has been processed\n",
      "Tweets 248 of 359 has been processed\n",
      "Tweets 249 of 359 has been processed\n",
      "Tweets 250 of 359 has been processed\n",
      "Tweets 251 of 359 has been processed\n",
      "Tweets 252 of 359 has been processed\n",
      "Tweets 253 of 359 has been processed\n",
      "Tweets 254 of 359 has been processed\n",
      "Tweets 255 of 359 has been processed\n",
      "Tweets 256 of 359 has been processed\n",
      "Tweets 257 of 359 has been processed\n",
      "Tweets 258 of 359 has been processed\n",
      "Tweets 259 of 359 has been processed\n",
      "Tweets 260 of 359 has been processed\n",
      "Tweets 261 of 359 has been processed\n",
      "Tweets 262 of 359 has been processed\n",
      "Tweets 263 of 359 has been processed\n",
      "Tweets 264 of 359 has been processed\n",
      "Tweets 265 of 359 has been processed\n",
      "Tweets 266 of 359 has been processed\n",
      "Tweets 267 of 359 has been processed\n",
      "Tweets 268 of 359 has been processed\n",
      "Tweets 269 of 359 has been processed\n",
      "Tweets 270 of 359 has been processed\n",
      "Tweets 271 of 359 has been processed\n",
      "Tweets 272 of 359 has been processed\n",
      "Tweets 273 of 359 has been processed\n",
      "Tweets 274 of 359 has been processed\n",
      "Tweets 275 of 359 has been processed\n",
      "Tweets 276 of 359 has been processed\n",
      "Tweets 277 of 359 has been processed\n",
      "Tweets 278 of 359 has been processed\n",
      "Tweets 279 of 359 has been processed\n",
      "Tweets 280 of 359 has been processed\n",
      "Tweets 281 of 359 has been processed\n",
      "Tweets 282 of 359 has been processed\n",
      "Tweets 283 of 359 has been processed\n",
      "Tweets 284 of 359 has been processed\n",
      "Tweets 285 of 359 has been processed\n",
      "Tweets 286 of 359 has been processed\n",
      "Tweets 287 of 359 has been processed\n",
      "Tweets 288 of 359 has been processed\n",
      "Tweets 289 of 359 has been processed\n",
      "Tweets 290 of 359 has been processed\n",
      "Tweets 291 of 359 has been processed\n",
      "Tweets 292 of 359 has been processed\n",
      "Tweets 293 of 359 has been processed\n",
      "Tweets 294 of 359 has been processed\n",
      "Tweets 295 of 359 has been processed\n",
      "Tweets 296 of 359 has been processed\n",
      "Tweets 297 of 359 has been processed\n",
      "Tweets 298 of 359 has been processed\n",
      "Tweets 299 of 359 has been processed\n",
      "Tweets 300 of 359 has been processed\n",
      "Tweets 301 of 359 has been processed\n",
      "Tweets 302 of 359 has been processed\n",
      "Tweets 303 of 359 has been processed\n",
      "Tweets 304 of 359 has been processed\n",
      "Tweets 305 of 359 has been processed\n",
      "Tweets 306 of 359 has been processed\n",
      "Tweets 307 of 359 has been processed\n",
      "Tweets 308 of 359 has been processed\n",
      "Tweets 309 of 359 has been processed\n",
      "Tweets 310 of 359 has been processed\n",
      "Tweets 311 of 359 has been processed\n",
      "Tweets 312 of 359 has been processed\n",
      "Tweets 313 of 359 has been processed\n",
      "Tweets 314 of 359 has been processed\n",
      "Tweets 315 of 359 has been processed\n",
      "Tweets 316 of 359 has been processed\n",
      "Tweets 317 of 359 has been processed\n",
      "Tweets 318 of 359 has been processed\n",
      "Tweets 319 of 359 has been processed\n",
      "Tweets 320 of 359 has been processed\n",
      "Tweets 321 of 359 has been processed\n",
      "Tweets 322 of 359 has been processed\n",
      "Tweets 323 of 359 has been processed\n",
      "Tweets 324 of 359 has been processed\n",
      "Tweets 325 of 359 has been processed\n",
      "Tweets 326 of 359 has been processed\n",
      "Tweets 327 of 359 has been processed\n",
      "Tweets 328 of 359 has been processed\n",
      "Tweets 329 of 359 has been processed\n",
      "Tweets 330 of 359 has been processed\n",
      "Tweets 331 of 359 has been processed\n",
      "Tweets 332 of 359 has been processed\n",
      "Tweets 333 of 359 has been processed\n",
      "Tweets 334 of 359 has been processed\n",
      "Tweets 335 of 359 has been processed\n",
      "Tweets 336 of 359 has been processed\n",
      "Tweets 337 of 359 has been processed\n",
      "Tweets 338 of 359 has been processed\n",
      "Tweets 339 of 359 has been processed\n",
      "Tweets 340 of 359 has been processed\n",
      "Tweets 341 of 359 has been processed\n",
      "Tweets 342 of 359 has been processed\n",
      "Tweets 343 of 359 has been processed\n",
      "Tweets 344 of 359 has been processed\n",
      "Tweets 345 of 359 has been processed\n",
      "Tweets 346 of 359 has been processed\n",
      "Tweets 347 of 359 has been processed\n",
      "Tweets 348 of 359 has been processed\n",
      "Tweets 349 of 359 has been processed\n",
      "Tweets 350 of 359 has been processed\n",
      "Tweets 351 of 359 has been processed\n",
      "Tweets 352 of 359 has been processed\n",
      "Tweets 353 of 359 has been processed\n",
      "Tweets 354 of 359 has been processed\n",
      "Tweets 355 of 359 has been processed\n",
      "Tweets 356 of 359 has been processed\n",
      "Tweets 357 of 359 has been processed\n",
      "Tweets 358 of 359 has been processed\n",
      "Tweets 359 of 359 has been processed\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaning and parsing the tweets...\\n\")\n",
    "clean_tweet_test_texts = []\n",
    "\n",
    "\n",
    "for i in range(359):\n",
    "        \n",
    "    print(\"Tweets %d of 359 has been processed\" % ( i+1 ) )                                                                   \n",
    "    clean_tweet_test_texts.append(tweet_cleaner_updated2(dftestmod['text'][i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test=clean_tweet_test_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=clean_tweet_texts\n",
    "y=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y=[0 if i<5000 else 4 for i in range(10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tbresult = [TextBlob(i).sentiment.polarity for i in X]\n",
    "tbpred = [0 if n < 0 else 1 for n in tbresult]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matrix=confusion_matrix(y, tbpred, labels=[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dftrain[:800000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test=dftestmod.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=80000, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words={'about', 'were', 'the', 'been', 'for', \"weren't\", 'until', 'up', 'now', 'd', 'does', \"hasn't\", 'very', 'her', 'shouldn', 'with', 'then', 'whom', 'no', \"you'd\", 'after', 'me', 'there', 'both', 'is', \"you've\", 'are', 'mightn', 'can', 'but', 'over', \"hadn't\", 'mustn', 'just', 'himself', 'th... 'having', 'as', \"wasn't\", 'so', 'our', 'weren', 'theirs', 'haven', 'was', 'she', 'hasn', 'herself'},\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for n in range(10000,1000001,10000):\n",
    "vectorizer.set_params(max_features=80000,stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have a new cat'"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"I hzve a new cat\").correct().string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vectorizer.fit(X)\n",
    "X_2 = vectorizer.transform(X)\n",
    "X_test_2 = vectorizer.transform(X_test)\n",
    "\n",
    "X_train_2, X_val_2, y_train_2, y_val_2 = train_test_split(\n",
    "    X_2, y, train_size = 0.75\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.5: 0.716\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C=0.05)\n",
    "lr.fit(X_train_2, y_train_2)\n",
    "print (\"Accuracy for C=%s: %s\" % (0.5, accuracy_score(y_val_2, lr.predict(X_val_2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.732590529248\n"
     ]
    }
   ],
   "source": [
    "final_model = LogisticRegression(C=0.05)\n",
    "final_model.fit(X_2, y)\n",
    "\n",
    "print (\"Final Accuracy: %s\" % accuracy_score(y_test, final_model.predict(X_test_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stemmed_text(corpus):\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    return [' '.join([stemmer.stem(word) for word in tweet1.split()]) for tweet1 in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmed_tweet_train = stemmed_text(X)\n",
    "stemmed_tweet_test = stemmed_text(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(stemmed_tweet_train)\n",
    "X_stemmed = cv.transform(stemmed_tweet_train)\n",
    "X_test_stemmed = cv.transform(stemmed_tweet_test)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_stemmed, y, train_size = 0.75\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.7016\n",
      "Accuracy for C=0.05: 0.738\n",
      "Accuracy for C=0.25: 0.754\n",
      "Accuracy for C=0.5: 0.7524\n",
      "Accuracy for C=1: 0.7508\n"
     ]
    }
   ],
   "source": [
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.738161559889\n"
     ]
    }
   ],
   "source": [
    "final_stemmed = LogisticRegression(C=0.25)\n",
    "final_stemmed.fit(X_stemmed, y)\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(y_test, final_stemmed.predict(X_test_stemmed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatized_text(corpus):\n",
    "    from nltk.stem import wordnet    \n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [' '.join([lemmatizer.lemmatize(word) for word in tweet.split()]) for tweet in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lemmatized_tweet_train = lemmatized_text(X)\n",
    "lemmatized_tweet_test = lemmatized_text(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(lemmatized_tweet_train)\n",
    "X_lemmatized = cv.transform(lemmatized_tweet_train)\n",
    "X_test_lemmatized = cv.transform(lemmatized_tweet_test)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_lemmatized, y, train_size = 0.75\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.702\n",
      "Accuracy for C=0.05: 0.7328\n",
      "Accuracy for C=0.25: 0.754\n",
      "Accuracy for C=0.5: 0.7504\n",
      "Accuracy for C=1: 0.7436\n"
     ]
    }
   ],
   "source": [
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.735376044568\n"
     ]
    }
   ],
   "source": [
    "final_lemmatized = LogisticRegression(C=0.1)\n",
    "final_lemmatized.fit(X_lemmatized, y)\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(y_test, final_lemmatized.predict(X_test_lemmatized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))\n",
    "ngram_vectorizer.fit(stemmed_tweet_train)\n",
    "X_stemmed_ngram1_2 = ngram_vectorizer.transform(stemmed_tweet_train)\n",
    "X_test_stemmed_ngram1_2 = ngram_vectorizer.transform(stemmed_tweet_test)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_stemmed_ngram1_2, y, train_size = 0.75\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.7216\n",
      "Accuracy for C=0.05: 0.7468\n",
      "Accuracy for C=0.25: 0.7572\n",
      "Accuracy for C=0.5: 0.7604\n",
      "Accuracy for C=1: 0.756\n"
     ]
    }
   ],
   "source": [
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.743732590529\n"
     ]
    }
   ],
   "source": [
    "#for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "final_ngram = LogisticRegression(C=0.5)\n",
    "final_ngram.fit(X_stemmed_ngram1_2, y)\n",
    "print (\"Final Accuracy: %s\" \n",
    "% accuracy_score(y_test, final_ngram.predict(X_test_stemmed_ngram1_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))\n",
    "ngram_vectorizer.fit(lemmatized_tweet_train)\n",
    "X_lemmatized_ngram1_2 = ngram_vectorizer.transform(lemmatized_tweet_train)\n",
    "X_test_lemmatized_ngram1_2 = ngram_vectorizer.transform(lemmatized_tweet_test)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_lemmatized_ngram1_2, y, train_size = 0.75\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.7328\n",
      "Accuracy for C=0.05: 0.7572\n",
      "Accuracy for C=0.25: 0.7648\n",
      "Accuracy for C=0.5: 0.768\n",
      "Accuracy for C=1: 0.7684\n"
     ]
    }
   ],
   "source": [
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.75208913649\n"
     ]
    }
   ],
   "source": [
    "#for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "final_ngram = LogisticRegression(C=0.5)\n",
    "final_ngram.fit(X_lemmatized_ngram1_2, y)\n",
    "print (\"Final Accuracy: %s\" \n",
    "% accuracy_score(y_test, final_ngram.predict(X_test_lemmatized_ngram1_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 3))\n",
    "vectorizer.set_params(max_features=80000,stop_words=stop_words)\n",
    "ngram_vectorizer.fit(lemmatized_tweet_train)\n",
    "X_lemmatized_ngram1_2 = ngram_vectorizer.transform(lemmatized_tweet_train)\n",
    "X_test_lemmatized_ngram1_2 = ngram_vectorizer.transform(lemmatized_tweet_test)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_lemmatized_ngram1_2, y, train_size = 0.75\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=1: 0.7336\n",
      "Accuracy for C=2: 0.736\n",
      "Accuracy for C=3: 0.7368\n",
      "Accuracy for C=4: 0.7332\n",
      "Accuracy for C=5: 0.7312\n",
      "Accuracy for C=6: 0.726\n",
      "Accuracy for C=7: 0.726\n",
      "Accuracy for C=8: 0.7244\n",
      "Accuracy for C=9: 0.7232\n",
      "Accuracy for C=10: 0.7216\n",
      "Accuracy for C=11: 0.718\n",
      "Accuracy for C=12: 0.718\n",
      "Accuracy for C=13: 0.7168\n",
      "Accuracy for C=14: 0.7144\n"
     ]
    }
   ],
   "source": [
    "for c in range(1,15):\n",
    "    \n",
    "    lr = MultinomialNB(alpha=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.766016713092\n"
     ]
    }
   ],
   "source": [
    "#for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "final_ngram = MultinomialNB(alpha=5)\n",
    "final_ngram.fit(X_lemmatized_ngram1_2, y)\n",
    "print (\"Final Accuracy: %s\" \n",
    "% accuracy_score(y_test, final_ngram.predict(X_test_lemmatized_ngram1_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1,3))\n",
    "vectorizer.set_params(max_features=80000,stop_words=stop_words)\n",
    "ngram_vectorizer.fit(lemmatized_tweet_train)\n",
    "X_lemmatized_ngram1_2 = ngram_vectorizer.transform(lemmatized_tweet_train)\n",
    "X_test_lemmatized_ngram1_2 = ngram_vectorizer.transform(lemmatized_tweet_test)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_lemmatized_ngram1_2, y, train_size = 0.75\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.7584\n",
      "Accuracy for C=0.05: 0.7644\n",
      "Accuracy for C=0.25: 0.752\n",
      "Accuracy for C=0.5: 0.7428\n",
      "Accuracy for C=1: 0.7348\n",
      "Accuracy for C=2: 0.7256\n",
      "Accuracy for C=3: 0.7176\n",
      "Accuracy for C=4: 0.7148\n",
      "Accuracy for C=5: 0.71\n",
      "Accuracy for C=6: 0.7064\n",
      "Accuracy for C=7: 0.7056\n",
      "Accuracy for C=8: 0.702\n",
      "Accuracy for C=9: 0.6984\n",
      "Accuracy for C=10: 0.698\n"
     ]
    }
   ],
   "source": [
    "for c in [0.01, 0.05, 0.25, 0.5, 1,2,3,4,5,6,7,8,9,10]:\n",
    "    \n",
    "    lr = LinearSVC(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.74651810585\n"
     ]
    }
   ],
   "source": [
    "#for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "final_ngram =LinearSVC(C=0.05)\n",
    "final_ngram.fit(X_lemmatized_ngram1_2, y)\n",
    "print (\"Final Accuracy: %s\" \n",
    "% accuracy_score(y_test, final_ngram.predict(X_test_lemmatized_ngram1_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-529-a84f3ea0be05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m feature_to_coef = {\n\u001b[0;32m      2\u001b[0m     word: coef for word, coef in zip(\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mngram_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     )\n\u001b[0;32m      5\u001b[0m }\n",
      "\u001b[1;31mNameError\u001b[0m: name 'final' is not defined"
     ]
    }
   ],
   "source": [
    "feature_to_coef = {\n",
    "    word: coef for word, coef in zip(\n",
    "        ngram_vectorizer.get_feature_names(), final.coef_[0]\n",
    "    )\n",
    "}\n",
    "\n",
    "for best_positive in sorted(\n",
    "    feature_to_coef.items(), \n",
    "    key=lambda x: x[1], \n",
    "    reverse=True)[:30]:\n",
    "    print (best_positive)\n",
    "    \n",
    "print(\"\\n\\n\")\n",
    "for best_negative in sorted(\n",
    "    feature_to_coef.items(), \n",
    "    key=lambda x: x[1])[:30]:\n",
    "    print (best_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
